{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "print(df.columns)\n",
    "if 'review' not in df.columns or 'sentiment' not in df.columns:\n",
    "    raise ValueError(\"Dataset must have 'review' and 'sentiment' columns.\")\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "print(df[['review', 'cleaned_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize it to make the machine learn\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "x = vectorizer.fit_transform(df['cleaned_review'])\n",
    "y = df['sentiment']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {x_train.shape}, Test size: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "model1=log_reg.fit(x_train, y_train)\n",
    "y_pred_log = model1.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_log)\n",
    "precision = precision_score(y_test, y_pred_log)\n",
    "recall = recall_score(y_test, y_pred_log)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(range(50), y_test.values[:50], label=\"Actual\", alpha=0.7, color='blue', marker='o')\n",
    "plt.scatter(range(50), y_pred_log[:50], label=\"Predicted\", alpha=0.7, color='red', marker='x')\n",
    "plt.title(\"Logistic Regression: Actual vs Predicted\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes (Multinomial)\n",
    "nb = MultinomialNB()\n",
    "model2=nb.fit(x_train, y_train)\n",
    "y_pred_nb = model2.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "precision = precision_score(y_test, y_pred_nb)\n",
    "recall = recall_score(y_test, y_pred_nb)\n",
    "\n",
    "print(f\"Naive Bayes - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(range(50), y_test.values[:50], label=\"Actual\", alpha=0.7, color='blue', marker='o')\n",
    "plt.scatter(range(50), y_pred_nb[:50], label=\"Predicted\", alpha=0.7, color='red', marker='x')\n",
    "plt.title(\"Logistic Regression: Actual vs Predicted\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Hyper Parameter Tuning\n",
    "log_reg_params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "model3 = log_reg_grid.fit(x_train, y_train)\n",
    "log_reg_pred_ht=model3.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, log_reg_pred_ht)\n",
    "precision = precision_score(y_test, log_reg_pred_ht)\n",
    "recall = recall_score(y_test, log_reg_pred_ht)\n",
    "print(f\"Logistic Regression HyperParameter Tuned - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "print(f\"Best Logistic Regression Params: {model3.best_params_}\")\n",
    "print(f\"Best Logistic Regression Accuracy: {model3.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(range(50), y_test.values[:50], label=\"Actual\", alpha=0.7, color='blue', marker='o')\n",
    "plt.scatter(range(50), log_reg_pred_ht[:50], label=\"Predicted\", alpha=0.7, color='red', marker='x')\n",
    "plt.title(\"Logistic Regression: Actual vs Predicted\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Hyper Parameter Tuning\n",
    "\n",
    "nb_params = {\n",
    "    'alpha': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "nb = MultinomialNB()\n",
    "nb_grid = GridSearchCV(nb, nb_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "model4 = nb_grid.fit(x_train, y_train)\n",
    "nb_pred_ht=model4.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, nb_pred_ht)\n",
    "precision = precision_score(y_test, nb_pred_ht)\n",
    "recall = recall_score(y_test, nb_pred_ht)\n",
    "print(f\"Naive Bayes Hyperparameter Tuned - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "print(f\"Best Naive Bayes Params: {model4.best_params_}\")\n",
    "print(f\"Best Naive Bayes Accuracy: {model4.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(range(50), y_test.values[:50], label=\"Actual\", alpha=0.7, color='blue', marker='o')\n",
    "plt.scatter(range(50), nb_pred_ht[:50], label=\"Predicted\", alpha=0.7, color='red', marker='x')\n",
    "plt.title(\"Logistic Regression: Actual vs Predicted\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def classify_review(review):\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    transformed_review = vectorizer.transform([cleaned_review])\n",
    "    log_reg_pred = model1.predict(transformed_review)[0]\n",
    "    nb_pred = model2.predict(transformed_review)[0]\n",
    "    log_reg_ht_pred = model3.predict(transformed_review)[0]\n",
    "    nb_ht_pred = model4.predict(transformed_review)[0]\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    print(\"\\n--- Sentiment Classification Results ---\")\n",
    "    print(f\"Logistic Regression: {sentiment_map[log_reg_pred]}\")\n",
    "    print(f\"Naïve Bayes: {sentiment_map[nb_pred]}\")\n",
    "    print(f\"Logistic Regression (Hyperparameter Tuned): {sentiment_map[log_reg_ht_pred]}\")\n",
    "    print(f\"Naïve Bayes (Hyperparameter Tuned): {sentiment_map[nb_ht_pred]}\")\n",
    "user_review = input(\"Enter a review: \")\n",
    "print(f\"Review: {user_review}\")\n",
    "classify_review(user_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
